{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
    "\n",
    "* it specifies an intuitive metric (the log-odds) for comparing word probabilities in two corpora\n",
    "* it incorporates prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
    "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
    "\n",
    "In this homework you will implement both of these ratios and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, operator, math, nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize(filename):\n",
    "    \n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        tokens=[]\n",
    "        # lowercase\n",
    "        for line in file:\n",
    "            data=line.rstrip().lower()\n",
    "            # This dataset is already tokenized, so we can split on whitespace\n",
    "            tokens.extend(data.split(\" \"))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll use in this case comes from a sample of 1000 positive and 1000 negative movie reviews from the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).  The version of the data used in this homework has already been tokenized for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tokens=read_and_tokenize(\"../data/negative.reviews.txt\")\n",
    "positive_tokens=read_and_tokenize(\"../data/positive.reviews.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.  Implement the log-odds ratio with an uninformative Dirichlet prior.  This value, $\\hat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\hat\\zeta_w^{(i-j)}= {\\hat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "\n",
    "$$\n",
    "\\hat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
    "* $\\alpha_w$ = 0.01\n",
    "* $V$ = size of vocabulary (number of distinct word types)\n",
    "* $\\alpha_0 = V * \\alpha_w$\n",
    "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
    "\n",
    "Here the two corpora are the positive movie reviews (e.g., $i$ = positive) and the negative movie reviews (e.g., $j$ = negative). Using this metric, print out the 25 words most strongly aligned with the positive corpus, and 25 words most strongly aligned with the negative corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logodds_with_uninformative_prior(one_tokens, two_tokens, display=25):\n",
    "    \n",
    "    # complete this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logodds_with_uninformative_prior(positive_tokens, negative_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: As you increase the constant $\\alpha_w$ in the equations above, what happens to $\\hat\\zeta_w^{(i-j)}$, $\\hat{d}_w^{(i-j)}$ and $\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)$ (i.e., do they get bigger or smaller)?  Answer this by plugging the following values in your implementation of these two quantities, and varying $\\alpha_w$ (and, consequently, $\\alpha_0$).\n",
    "\n",
    "* $y_w^i=34$\n",
    "* $y_w^j=17$\n",
    "* $n^i=1000$\n",
    "* $n^j=1000$\n",
    "* $V=500$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make that prior informative by including information about the overall frequency of a given word in a background corpus (i.e., a corpus that represents general word usage, without regard for labeled subcorpora).  To do so, there are only two small changes to make:\n",
    "\n",
    "* We need to gather a background corpus $b$ and calculate $\\hat\\pi_w$, the relative frequency of word $w$ in $b$ (i.e., the number of times $w$ occurs in $b$ divided by the number of words in $b$).\n",
    "\n",
    "* In the uninformative prior above, $\\alpha_w$ was a constant (0.01) and $\\alpha_0 = V * \\alpha_w$.  Let us now set $\\alpha_0 = 1000$ and $\\alpha_w = \\hat\\pi_w * \\alpha_0$.  This reflects a pseudocount capturing the fractional number of times we would expect to see word $w$ in a sample of 1000 words.\n",
    "\n",
    "This allows us to specify that a common word like \"the\" (which has a relative frequency of $\\approx 0.04$) would have $\\alpha_w = 40$, while an infrequent word like \"beneficiaries\" (relative frequency $\\approx 0.00002$) would have $\\alpha_w = 0.02$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Implement a log-odds ratio with informative prior, using a larger background corpus of 5M tokens drawn from the same dataset (given to you as `priors` below, which contains the relative frequencies of words calculated from that corpus) and set $\\alpha_0 = 1000$. Using this metric, print out again the 25 words most strongly aligned with the positive corpus, and 25 words most strongly aligned with the negative corpus.  Is there a meaningful difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_priors(filename):\n",
    "    counts=Counter()\n",
    "    freqs={}\n",
    "    tokens=read_and_tokenize(filename)\n",
    "    total=len(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        counts[token]+=1\n",
    "\n",
    "    for word in counts:\n",
    "        freqs[word]=counts[word]/total\n",
    "\n",
    "    return freqs\n",
    "    \n",
    "priors=read_priors(\"../data/sentiment.background.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logodds_with_informative_prior(one_tokens, two_tokens, priors, display=25):\n",
    "    \n",
    "    # complete this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logodds_with_informative_prior(positive_tokens, negative_tokens, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
